apiVersion: ray.io/v1
kind: RayService
metadata:
  name: finetune-sample
  namespace: datatunerx-dev
  labels:
    createdByFrontend: "true"
spec:
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: 0.0.0.0
        num-gpus: "0"
      serviceType: NodePort
      template:
        spec:
          containers:
            - name: ray-head
              image: 10.33.1.11:9090/datatunerx/ray271-llama2-7b-finetune-checkpoint-job1:20231206
              ports:
                - containerPort: 6379
                  name: gcs-server
                  protocol: TCP
                - containerPort: 8265
                  name: dashboard
                  protocol: TCP
                - containerPort: 10001
                  name: client
                  protocol: TCP
                - containerPort: 8000
                  name: serve
                  protocol: TCP
              resources:
                limits:
                  cpu: 2000m
                  memory: 8Gi
                requests:
                  cpu: 1000m
                  memory: 4Gi
    rayVersion: 2.7.1
    workerGroupSpecs:
      - groupName: group
        rayStartParams: {}
        maxReplicas: 1
        minReplicas: 1
        replicas: 1
        template:
          spec:
            containers:
              - name: 'ray-worker'
                env:
                  - name: "BASE_MODEL_DIR"
                    value: "/tmp/llama2-7b/"
                  - name: "CHECKPOINT_DIR"
                    value: "/checkpoint/tuning/TorchTrainer_2023-12-05_20-36-02/TorchTrainer_02738_00000_0_2023-12-05_20-36-29/checkpoint_000000"
                image: 10.33.1.11:9090/datatunerx/ray271-llama2-7b-finetune-checkpoint-job1:20231206
                lifecycle:
                  preStop:
                    exec:
                      command:
                        - /bin/sh
                        - -c
                        - ray stop
                resources:
                  limits:
                    cpu: 8000m
                    memory: 48Gi
                    nvidia.com/gpu: "1"
                  requests:
                    cpu: 1000m
                    memory: 48Gi
  serveConfig:
    deployments:
      - name: LlamaDeployment
        numReplicas: 1
        rayActorOptions:
          numGpus: 1
    importPath: inference.deployment
    runtimeEnv: |
      working_dir: file:///home/inference/inference.zip
  serveService:
    metadata:
      labels:
        app: inference
      name: finetune-sample-service
    spec:
      ports:
        - name: serve
          port: 8000
          protocol: TCP
          targetPort: 8000
      selector:
        ray.io/node-type: head
